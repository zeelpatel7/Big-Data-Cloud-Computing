# Big Data and Cloud Computing

This repository is dedicated to exploring Big Data concepts and applying large-scale data mining techniques to real-world scenarios. The content revolves around utilizing popular Big Data tools and cloud infrastructure, providing hands-on examples and projects to showcase approaches like distributed computing, parallel processing, and cloud-based data engineering. Examples using Hadoop and Spark for data processing, Spark RDD and DataFrames for data analysis, as well as various cloud computing environments like Google Cloud Platform (GCP), AWS, and Azure are included.

The projects illustrate the practical use of Big Data infrastructure including Linux, containerization with Docker and Kubernetes, and distributed file systems like HDFS. The application of NoSQL systems, such as Elasticsearch and JSON stores, to work with unstructured data is also covered. The repository includes end-to-end data science workflows: data engineering, building machine learning models, and deploying models in a containerized cloud environment for scalability.

Skills and topics covered in this repository:

Proficiency with distributed computing frameworks like Hadoop and Spark for data processing and analysis.

Experience working with Spark RDDs and DataFrames to handle large datasets efficiently.

Familiarity with cloud platforms such as GCP, AWS, and Azure, including setting up and managing cloud infrastructure for Big Data projects.

Knowledge of Linux for managing cloud-based environments and executing large-scale data processing tasks.

Understanding of containerization technologies like Docker and Kubernetes for deploying applications with scalability and ease of management.

Practical experience with NoSQL databases, including Elasticsearch and JSON stores, for managing unstructured data.

Building machine learning models (e.g., clustering, classification, regression) and deploying them in cloud environments.

Utilizing tools like Hive, Python, and PySpark for Big Data applications in a client-server environment.

Working with virtualization and container orchestration to manage and scale Big Data solutions effectively.



#### 1. Amazon Reviews with SparkDF and Big Data on GCP Dataproc
This notebook focuses on analyzing Amazon product reviews using Spark DataFrames and GCP Dataproc. The analysis includes data cleaning (removing null values and cleaning product categories), identifying the product category with the most reviews, analyzing star ratings, and examining seasonality trends across different decades. It also explores the consistency of reviews by category and identifies patterns related to the timing of reviews, providing insights into consumer behavior and product review dynamics over time.

#### 2. Data Engineering with Spark Dataframes
This notebook demonstrates various data engineering techniques using Spark DataFrames. The primary tasks involve data preprocessing, data transformation, and feature extraction. The notebook also showcases exploratory data analysis, including insights into delays and the distribution of flights across different times of the day. Analyses include worst delays by time, weekend effects, and a comparison of flight activity between 2007 and 2008. Visualizations help understand patterns in flight delays and operational behavior over time.

#### 3. Data Exploration and Preparation Using Spark RDD
This notebook covers data exploration and preparation using Spark RDDs. The steps include copying and unzipping a dataset, reading data into Spark RDD, and processing the header record. The analysis involves calculating the frequency of offenses by offense case type and identifying the most frequent offenses by charge description. The notebook demonstrates foundational techniques for processing large datasets and generating meaningful frequency-based insights.
